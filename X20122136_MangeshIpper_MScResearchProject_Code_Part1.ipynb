{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                 x20122136_ResearchProject_HKDataset_Part_1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Objective](#section1)<br>\n",
    "2. [Importing Packages](#section2)<br>\n",
    "3. [Loading Runs and Races CSV's](#section3)<br>\n",
    "4. [EDA](#section4)<br>\n",
    "  - 4.1 [EDA report on reaces Data](#section401)<br>\n",
    "  - 4.2 [EDA report on runs Data](#section402)<br>\n",
    "5. [Data Pre-Processing](#section5)\n",
    "  - 5.1 [Prepare races data from races.csv](#section501)<br>\n",
    "  - 5.2 [Prepare races data from runs.csv](#section502)<br>\n",
    "  - 5.3 [Further preprocessing for runs data](#section503)<br>\n",
    "  <br>\n",
    "6. [Prepare training and test data](#section6)<br>\n",
    "7. [ANN algorithm](#section7)\n",
    "  - 7.1 [ Build the model ](#section701)<br>\n",
    "  - 7.2 [Train the model ](#section702)<br>\n",
    "  <br>\n",
    "8. [Visualization ](#section8)<br>\n",
    "9. [Conclusion ](#section9)<br>\n",
    "  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective<a id=section1></a>\n",
    "\n",
    "In this notebook applied deep learning algorithm on HK dataset to extend its accuracy using ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import packages <a id=section2></a>\n",
    "Here, we import common packages for deep learning. \n",
    "- pandas: for data reading and preprocessing\n",
    "- tensorflow: for neural network construction \n",
    "- sklearn.preprocessing: for data encoding\n",
    "- sklearn.model_selection: it has convenient method for training/test data spliting \n",
    "- matplotlib.pyplot: to plot performance of the training process.\n",
    "- pandas_profiling: to perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Runs and Races CSV's <a id=section3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV's nd run EDA on them to understand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T21:35:20.188053Z",
     "iopub.status.busy": "2021-12-13T21:35:20.187760Z",
     "iopub.status.idle": "2021-12-13T21:35:20.621390Z",
     "shell.execute_reply": "2021-12-13T21:35:20.620573Z",
     "shell.execute_reply.started": "2021-12-13T21:35:20.188006Z"
    }
   },
   "outputs": [],
   "source": [
    "races_df = pd.read_csv(r\"races.csv\", delimiter=\",\", header=0, index_col='race_id')\n",
    "runs_df = pd.read_csv(r\"runs.csv\", delimiter=\",\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA <a id=section4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. EDA report on reaces Data <a id=section401></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T21:35:22.173088Z",
     "iopub.status.busy": "2021-12-13T21:35:22.172811Z",
     "iopub.status.idle": "2021-12-13T21:35:22.177554Z",
     "shell.execute_reply": "2021-12-13T21:35:22.176715Z",
     "shell.execute_reply.started": "2021-12-13T21:35:22.173056Z"
    }
   },
   "outputs": [],
   "source": [
    "# races_profile = ProfileReport(races_df, title=\"Pandas Profiling Races Report\")\n",
    "\n",
    "# races_profile.to_file(\"/kaggle/working/eda_races.html\")\n",
    "\n",
    "# races_profile.to_file(\"/kaggle/working/eda_races.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 EDA report on runs Data <a id=section402></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T21:35:23.116424Z",
     "iopub.status.busy": "2021-12-13T21:35:23.115995Z",
     "iopub.status.idle": "2021-12-13T21:35:23.121080Z",
     "shell.execute_reply": "2021-12-13T21:35:23.120164Z",
     "shell.execute_reply.started": "2021-12-13T21:35:23.116387Z"
    }
   },
   "outputs": [],
   "source": [
    "# runs_profile = ProfileReport(runs_df, title=\"Pandas Profiling Races Report\")\n",
    "\n",
    "# runs_profile.to_file(\"/kaggle/working/eda_runs.html\")\n",
    "\n",
    "# runs_profile.to_file(\"/kaggle/working/eda_runs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Pre-Processing <a id=section5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found it makes more sense to predict winner horse for every race because winning is **relative** to other horses performance rather predict every single horse run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Prepare races data from races.csv <a id=section501></a>\n",
    "Only select several columns that make sense for this kernel. Then, use different encoders for different types of attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "races_df = races_df[['venue', 'config', 'surface', 'distance', 'going', 'race_class']]\n",
    "\n",
    "# check to see if we have NaN, then drop NaN\n",
    "print(races_df[races_df.isnull().any(axis=1)])\n",
    "races_df = races_df.dropna()\n",
    "\n",
    "print(races_df.shape)\n",
    "print(races_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode ordinal columns: config, going, \n",
    "config_encoder = preprocessing.OrdinalEncoder()\n",
    "races_df['config'] = config_encoder.fit_transform(races_df['config'].values.reshape(-1, 1))\n",
    "going_encoder = preprocessing.OrdinalEncoder()\n",
    "races_df['going'] = going_encoder.fit_transform(races_df['going'].values.reshape(-1, 1))\n",
    "\n",
    "# encode nominal column: venue\n",
    "venue_encoder = preprocessing.LabelEncoder()\n",
    "races_df['venue'] = venue_encoder.fit_transform(races_df['venue'])\n",
    "\n",
    "print(races_df.dtypes)\n",
    "print(races_df.shape)\n",
    "print(races_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Prepare races data from runs.csv <a id=section502></a>\n",
    "Similar to races data, only select columns that are relevant to the model. \n",
    "\n",
    "### Data cleaning\n",
    "- two rows that includes NaN, so just drop them.\n",
    "- strange data for 'draw', e.g. 15. As we only deal with standard 14 horses racing, so let's drop it.\n",
    "\n",
    "### Encoding \n",
    "Then, use label encoders for 'horse_country' and 'horse_type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df = runs_df[['race_id', 'draw', \n",
    "                   'horse_age', 'horse_country', 'horse_type', 'horse_rating', 'declared_weight', 'actual_weight', 'win_odds', \n",
    "                   'result']] \n",
    "\n",
    "# check to see if we have NaN, then drop NaN\n",
    "print(runs_df[runs_df.isnull().any(axis=1)])\n",
    "runs_df = runs_df.dropna()\n",
    "\n",
    "# not sure why, but we got some strange draw in the dataset. Maximum shall be 14\n",
    "strange_draw_index = runs_df[runs_df['draw'] > 14].index\n",
    "# delete these row indexes from dataFrame\n",
    "runs_df = runs_df.drop(strange_draw_index)\n",
    "\n",
    "# encode nominal columns: horse_country, horse_type\n",
    "horse_country_encoder = preprocessing.LabelEncoder()\n",
    "runs_df['horse_country'] = horse_country_encoder.fit_transform(runs_df['horse_country'])\n",
    "horse_type_encoder = preprocessing.LabelEncoder()\n",
    "runs_df['horse_type'] = horse_type_encoder.fit_transform(runs_df['horse_type'])\n",
    "\n",
    "print(runs_df.dtypes)\n",
    "print(runs_df.shape)\n",
    "print(runs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Further preprocessing for runs data <a id=section503></a>\n",
    "We are targeting to put all the 14 horses' features into the one input, but it expands into multiple rows now. Luckily, pandas has a nice method called `pivot`. `pivot` aggregates horses data from multiple rows, which belongs to a single race, into one row. \n",
    "\n",
    "After `pivot`, some races may not have 14 horses, so let's fill NaN with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_horse_and_result(element):\n",
    "    if element[0] == 'result':\n",
    "        return 100 + element[1] # to make sure results are put near the end\n",
    "    else:\n",
    "        return element[1]   \n",
    "\n",
    "runs_df = runs_df.pivot(index='race_id', columns='draw', values=runs_df.columns[2:])\n",
    "rearranged_columns = sorted(list(runs_df.columns.values), key=group_horse_and_result)\n",
    "runs_df = runs_df[rearranged_columns]\n",
    "print(runs_df.head())\n",
    "\n",
    "# quite some NaNs appreared in the dataframe, reason is some races didnt have full 14 horses participating\n",
    "# fill with 0\n",
    "runs_df = runs_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare training and test data <a id=section6></a>\n",
    "Here, we combine races data and runs data by `join` two data frames above. \n",
    "\n",
    "### Standardization\n",
    "If you look at the data closely, if will find that features are not in the same scale, e.g. weight can go to 1000+. Standardize the data for to make training easier. \n",
    "\n",
    "### Select right columns for X, y\n",
    "- Select all the data except last 28 columns, because last 28 columns is about 'result' and 'won'\n",
    "- Select last 14 columns for y_won. Each row shall have one '1.0' and rest are 0. \n",
    "- Select second last 14 columns for y_top3. It used to the the column 'result', e.g. 1~14, which is horses' final positions when the race finishes. Apply a function to convert it to 1.0 if the horse is in top 3, else 0. \n",
    "\n",
    "### Split data into train/test sets\n",
    "\n",
    "sklearn comes with such a handy method `train_test_split`. We split the data as following:\n",
    "- 80% for training\n",
    "- 20% for testing(validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = races_df.join(runs_df, on='race_id', how='right')\n",
    "X = data[data.columns[:-14]] \n",
    "ss = preprocessing.StandardScaler()\n",
    "X = pd.DataFrame(ss.fit_transform(X),columns = X.columns)\n",
    "\n",
    "y_won = data[data.columns[-14:]].applymap(lambda x: 1.0 if 0.5 < x < 1.5 else 0.0) \n",
    "\n",
    "print(X.shape)\n",
    "print(y_won.shape)\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y_won, train_size=0.8, test_size=0.2, random_state=1)\n",
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ANN algorithm <a id=section7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Build the model <a id=section701></a>\n",
    "\n",
    "Use keras to build the model with easy-to-use api `Sequential`. \n",
    "\n",
    "Have to mention that input layer has 104 inputs. The calculation is following:\n",
    "- 6 features from races dataframe: 'venue', 'config', 'surface', 'distance', 'going', 'race_class'\n",
    "- 14 horses per races, and each horse has 7 features; 'horse_age', 'horse_country', 'horse_type', 'horse_rating', 'declared_weight', 'actual_weight', 'win_odds'\n",
    "- so total 104 features = 6 + 14 x 7\n",
    "\n",
    "\n",
    "Output layer has 14 nodes, as each node stands for each horse's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(96, activation='relu', input_shape=(104,)),\n",
    "    tf.keras.layers.Dense(14, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(5e-04),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.Precision(name='precision')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Train the model <a id=section702></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = dataset.shuffle(len(X_train)).batch(500)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "validation_dataset = dataset.shuffle(len(X_test)).batch(500)\n",
    "\n",
    "print(\"Start training..\\n\")\n",
    "history = model.fit(train_dataset, epochs=200, validation_data=validation_dataset)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization <a id=section8></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = history.history['precision']\n",
    "val_precision = history.history['val_precision']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(precision) + 1)\n",
    "\n",
    "plt.plot(epochs, precision, 'b', label='Training precision')\n",
    "plt.plot(epochs, val_precision, 'r', label='Validation precision')\n",
    "plt.title('Training and validation precision')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion <a id=section9></a>\n",
    "\n",
    "With the 2 layer nerual network, we reached 0.92 precision on the the training dataset. However, best precision on the testing dataset was about 0.3, which happened around epoch 70~80. Then overfitting happened. \n",
    "\n",
    "precision = 0.3, means If we bet 'Win' 10 times based on the model's prediction, only 3 times is correct.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
